# -*- coding: utf-8 -*-
"""HeartRatePredictor_V5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eu8-qkYYTEYY0WhLqg6bUEc1rAQouH77
"""

#KAGGLE DATASET: https://www.kaggle.com/datasets/arashnic/fitbit
# upload complete zip file to provide ample data for the model to work properly
from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Unzip uploaded file
with zipfile.ZipFile("archive.zip", 'r') as zip_ref:
    zip_ref.extractall("fitbit_data")

# List extracted files
print("Extracted files:")
print(os.listdir("fitbit_data"))

import pandas as pd
import os
import glob
import zipfile # Ensure zipfile is imported if it wasn't before

# Check if the archive.zip exists before attempting to extract
if not os.path.exists("archive.zip"):
    print("Error: archive.zip not found. Please upload the file.")
else:
    # Unzip uploaded file
    try:
        with zipfile.ZipFile("archive.zip", 'r') as zip_ref:
            zip_ref.extractall("fitbit_data")
        print(" archive.zip extracted successfully.")
    except zipfile.BadZipFile:
        print("Error: archive.zip is a bad zip file. Please re-upload.")
        # Exit or handle the error appropriately
    except FileExistsError:
         print(" fitbit_data directory already exists. Assuming extraction is complete.")
    except Exception as e:
        print(f"An unexpected error occurred during extraction: {e}")


# List extracted files
print("Extracted items in fitbit_data:")
try:
    extracted_items = os.listdir("fitbit_data")
    print(extracted_items)
except FileNotFoundError:
    print("Error: fitbit_data directory not found after extraction attempt.")
    extracted_items = [] # Set to empty list to avoid further errors


# Paths to base folders
base_folders = [
    "fitbit_data/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16",
    "fitbit_data/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16"
]

# Empty lists to store dataframes
steps_dfs = []
hr_dfs = []

print("\n--- Loading Data ---")
# Load all Step and HR files
for base in base_folders:
    print(f"\nChecking base folder: {base}")
    # Check if the base folder exists
    if not os.path.exists(base):
        print(f"   Base folder not found: {base}")
        continue # Skip to the next base folder

    print(f"   Base folder found: {base}")
    print(f"  Contents of base folder: {os.listdir(base)}")


    step_path_pattern = os.path.join(base, "Steps", "*.csv")
    hr_path_pattern = os.path.join(base, "Heart Rate", "*.csv")

    print(f"  Looking for Step files with pattern: {step_path_pattern}")
    step_files = glob.glob(step_path_pattern)
    print(f"  Found Step files: {step_files}")

    for file in step_files:
        try:
            df = pd.read_csv(file)
            df['SourceFile'] = os.path.basename(file)  # optional: keep user info
            steps_dfs.append(df)
            print(f"    Loaded {os.path.basename(file)}")
        except Exception as e:
            print(f"    Error loading {file}: {e}")


    print(f"  Looking for HR files with pattern: {hr_path_pattern}")
    hr_files = glob.glob(hr_path_pattern)
    print(f"  Found HR files: {hr_files}")

    for file in hr_files:
        try:
            df = pd.read_csv(file)
            df['SourceFile'] = os.path.basename(file)
            hr_dfs.append(df)
            print(f"    Loaded {os.path.basename(file)}")
        except Exception as e:
            print(f"    Error loading {file}: {e}")


print("\n--- Concatenation ---")
# Combine all into single DataFrames
if steps_dfs:
    all_steps = pd.concat(steps_dfs, ignore_index=True)
    print(" Combined Steps:", all_steps.shape)
    print(" Steps Columns:", all_steps.columns.tolist())
else:
    print("steps_dfs is empty. No steps data to combine.")
    all_steps = pd.DataFrame() # Create empty DataFrame to avoid potential errors later

if hr_dfs:
    all_hr = pd.concat(hr_dfs, ignore_index=True)
    print(" Combined Heart Rate:", all_hr.shape)
    print("🧾 HR Columns:", all_hr.columns.tolist())
else:
    print(" hr_dfs is empty. No heart rate data to combine.")
    all_hr = pd.DataFrame() # Create empty DataFrame to avoid potential errors later

#Print the folder structure
import os

for root, dirs, files in os.walk("fitbit_data"):
    level = root.replace("fitbit_data", "").count(os.sep)
    indent = " " * 2 * level
    print(f"{indent}{os.path.basename(root)}/")
    subindent = " " * 2 * (level + 1)
    for f in files:
        print(f"{subindent}{f}")

import pandas as pd
import os
import glob

# Correct base folders based on your structure
base_folders = [
    "fitbit_data/archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16",
    "fitbit_data/archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16"
]

# Load specific files by exact name (across both folders)
def load_csvs_across_periods(filename):
    dfs = []
    for base in base_folders:
        path = os.path.join(base, filename)
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['SourcePeriod'] = os.path.basename(base)  # Optional
            dfs.append(df)
    return pd.concat(dfs, ignore_index=True)

# Load combined datasets
steps_df = load_csvs_across_periods("dailySteps_merged.csv")
hr_df = load_csvs_across_periods("heartrate_seconds_merged.csv")

# Preview
print(" Combined Steps:", steps_df.shape)
print(" Combined Heart Rate:", hr_df.shape)
print(" Steps Columns:", steps_df.columns.tolist())
print(" HR Columns:", hr_df.columns.tolist())

# 🧠 Predict Next Week's Heart Health Using Complete Fitbit Data (30 Users, Two Time Periods)

# --- SETUP ---
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, r2_score
import os

# --- LOAD COMBINED CSV FILES ---
# Define base folders from both export periods
base_folders = [
    "fitbit_data/archive/mturkfitbit_export_3.12.16-4.11.16/Fitabase Data 3.12.16-4.11.16",
    "fitbit_data/archive/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16"
]

# Helper function to load and combine CSVs
def load_csvs_across_periods(filename):
    dfs = []
    for base in base_folders:
        path = os.path.join(base, filename)
        if os.path.exists(path):
            df = pd.read_csv(path)
            df['SourcePeriod'] = os.path.basename(base)
            dfs.append(df)
    return pd.concat(dfs, ignore_index=True)

# Load steps and heart rate data
steps_df = load_csvs_across_periods("dailySteps_merged.csv")
hr_df = load_csvs_across_periods("heartrate_seconds_merged.csv")
print("Loaded steps:", steps_df.shape)
print("Loaded HR:", hr_df.shape)

# --- PROCESS STEPS DATA ---
# Convert ActivityDay to datetime
steps_df['Date'] = pd.to_datetime(steps_df['ActivityDay'])
steps_df = steps_df.sort_values(by=['Id', 'Date'])

# Simulate week index per user based on sequence of days
steps_df['DayIndex'] = steps_df.groupby('Id').cumcount()
steps_df['WeekSim'] = steps_df['DayIndex'] // 7

# Group by user and week to get average weekly step count
weekly_steps = steps_df.groupby(['Id', 'WeekSim'])[['StepTotal']].mean().reset_index()
print("Weekly steps summary:", weekly_steps.shape)

# --- PROCESS HEART RATE DATA ---
# Convert Time to date
hr_df['Date'] = pd.to_datetime(hr_df['Time']).dt.date
hr_df['Date'] = pd.to_datetime(hr_df['Date'])

# Get daily HR average per user
daily_hr = hr_df.groupby(['Id', 'Date'])[['Value']].mean().reset_index()
daily_hr.rename(columns={'Value': 'HeartRate'}, inplace=True)
daily_hr = daily_hr.sort_values(by=['Id', 'Date'])

# Simulate week index per user for heart rate
daily_hr['DayIndex'] = daily_hr.groupby('Id').cumcount()
daily_hr['WeekSim'] = daily_hr['DayIndex'] // 7

# Group by user and week to get average HR
weekly_hr = daily_hr.groupby(['Id', 'WeekSim'])[['HeartRate']].mean().reset_index()
print("Weekly HR summary:", weekly_hr.shape)

# --- MERGE WEEKLY DATA ---
weekly_df = pd.merge(weekly_steps, weekly_hr, on=['Id', 'WeekSim'])
print("Merged weekly data:", weekly_df.shape)

# Shift HR values per user to get next week's HR as target
weekly_df = weekly_df.sort_values(by=['Id', 'WeekSim'])
weekly_df['NextWeekHR'] = weekly_df.groupby('Id')['HeartRate'].shift(-1)
weekly_df.dropna(inplace=True)
print("Final dataset after shift and dropna:", weekly_df.shape)

# --- PREPARE FEATURES AND TARGET ---
X = weekly_df[['StepTotal', 'HeartRate']]
y = weekly_df['NextWeekHR']

# Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Train samples:", len(X_train))
print("Test samples:", len(X_test))

# --- TRAIN MODELS ---
# Random Forest
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)

# Gradient Boosting
gb = GradientBoostingRegressor()
gb.fit(X_train, y_train)
gb_preds = gb.predict(X_test)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_preds = lr.predict(X_test)

# --- EVALUATE MODELS ---
def evaluate_model(name, y_true, y_pred):
    print(f"\n Evaluation: {name}")
    print("R² Score:", r2_score(y_true, y_pred))
    print("Mean Absolute Error:", mean_absolute_error(y_true, y_pred))

evaluate_model("Random Forest", y_test, rf_preds)
evaluate_model("Gradient Boosting", y_test, gb_preds)
evaluate_model("Linear Regression", y_test, lr_preds)

"""What This Means:
R² Score shows how much of the variation in next week's heart rate your model can explain.

R² = 1: perfect prediction

R² > 0.8: very good

MAE measures how far off the model is, on average, from the true values (in bpm).

👉 Gradient Boosting performed the best, with the highest R² and lowest MAE.

"""

# --- VISUALIZE PREDICTIONS ---
plt.figure(figsize=(10, 5))
plt.plot(y_test.values, label='Actual', linewidth=2)
plt.plot(rf_preds, label='Random Forest')
plt.plot(gb_preds, label='Gradient Boosting')
plt.plot(lr_preds, label='Linear Regression')
plt.title("Predicted vs Actual Heart Rate (Next Week)")
plt.xlabel("Sample")
plt.ylabel("Heart Rate")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""📊 Visualization Interpretation
Each point on the x-axis is a test sample (a week of data).
The y-axis shows the heart rate (bpm).

What to Look For:
Blue line ("Actual"): true average heart rate for the next week

Other lines: model predictions

Observations:
All models follow the general trend of the actual heart rate.

Gradient Boosting (green) and Random Forest (orange) stick closely to the actual line — especially during peaks and dips.

Linear Regression (red) also tracks the trend well but shows more overshooting or flattening in places.

🔍 Where It's Performing Well:
All models predict well when there’s a gradual trend.

Peak predictions (e.g., sample 4, 8) are fairly close.

⚠️ Where It Might Be Struggling:
Sudden drops or jumps (e.g., sample 6–7) are harder to track — especially for the Linear Regression model.

🧭 **Step-by-Step Explanation: Predicting Next Week's Heart Rate from Fitbit Data**
**1. Load the Raw Data**
You loaded data collected from 30 Fitbit users across two time periods. The data included:
•	Daily step counts
•	Second-by-second heart rate measurements
📦 This is your **raw input** from wearable devices.
________________________________________
**2. Convert Daily Data into Weekly Summaries**
Why?
•	You want to predict next week’s average heart rate based on this week’s activity.
•	So you summarized the data per user per **simulated week**:
o	Weekly average step count
o	Weekly average heart rate
📊 This step turns noisy daily data into a clean weekly format for modeling.
________________________________________
**3. Create the Prediction Target**
You created a new column called NextWeekHR, which represents:
The average heart rate for the following week
This allows your model to **learn patterns** like:
"If a user walks X steps and has Y heart rate this week, what will their average heart rate be next week?"
________________________________________
**4. Train ML Models**
You trained 3 different models:
•	**Random Forest**: uses many decision trees to make predictions
•	**Gradient Boosting**: improves accuracy by correcting mistakes in sequence
•	**Linear Regression**: draws a straight-line relationship between features and target
🎯 All models learned from:
•	This week's step count
•	This week's heart rate
To **predict next week's heart rate**.
________________________________________
**5. Evaluate the Models**
You used two common metrics:
•	**R² Score**: how well the model explains variation in the data
o	Closer to 1 means better prediction
•	**MAE (Mean Absolute Error)**: the average prediction error in bpm
o	Lower is better
✅ Your best model was **Gradient Boosting**:
•	R² ≈ 0.82 → explains ~82% of variation
•	MAE ≈ 2.6 bpm → on average, predictions are off by just 2.6 beats per minute
________________________________________
**6. Visualize the Predictions**
You created a line plot comparing:
•	🔵 The actual next-week heart rate
•	🟠🔴🟢 The predicted heart rates from each model
How to read it:
•	The closer the predicted lines are to the actual line, the better.
•	Your models followed the trend well — especially Gradient Boosting.
________________________________________
🔍 **What a Data Analyst or ML Engineer Understands from This**
They look at:
✅ **Metrics**
•	**High R² (0.82)** → The model is capturing the majority of heart rate behavior.
•	**Low MAE (2.6 bpm)** → Predictions are pretty close to real-world values.
✅ **Visualization**
•	All models track the actual line well → the trend is being captured.
•	Gradient Boosting is more accurate at sharp rises/falls.
________________________________________
💬 **So What Does This Mean?**
“Based on your step count and heart rate this week, our model (especially Gradient Boosting) can predict what your average heart rate is likely to be next week — usually within 2–3 beats of accuracy.”
💡 It does **not** predict **minute-by-minute** heart rate but gives a good estimate of your **overall weekly heart health trend**.
"""